🧩 Module 2 — Experiments and Evaluation

Goal: Conduct dataset-driven evaluations and experiments using LangSmith.

Video 1 — Datasets

Learned to create and manage datasets.

Added input-output schemas and AI-generated examples.

Video 2 — Evaluators

Created custom evaluators for automatic scoring.

Implemented “LLM-as-a-Judge” evaluation frameworks.

Video 3 — Experiments

Ran experiments comparing GPT-4o and GPT-3.5-turbo.

Conducted multiple runs using is_concise_enough evaluator.

Video 4 — Analyzing Results

Compared experiment outcomes and evaluated metrics.

Used dashboard analytics for result interpretation.

Video 5 — Pairwise Experiments

Performed head-to-head model comparisons.

Tested prompt optimization and evaluator calibration.

Video 6 — Summary Evaluators

Learned to compute F1 and precision-recall scores.

Analyzed aggregate performance (achieved F1 = 0.86).

Public Dataset:
🔗 LangSmith Dataset
