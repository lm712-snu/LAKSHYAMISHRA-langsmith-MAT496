ğŸ§© Module 2 â€” Experiments and Evaluation

Goal: Conduct dataset-driven evaluations and experiments using LangSmith.

Video 1 â€” Datasets

Learned to create and manage datasets.

Added input-output schemas and AI-generated examples.

Video 2 â€” Evaluators

Created custom evaluators for automatic scoring.

Implemented â€œLLM-as-a-Judgeâ€ evaluation frameworks.

Video 3 â€” Experiments

Ran experiments comparing GPT-4o and GPT-3.5-turbo.

Conducted multiple runs using is_concise_enough evaluator.

Video 4 â€” Analyzing Results

Compared experiment outcomes and evaluated metrics.

Used dashboard analytics for result interpretation.

Video 5 â€” Pairwise Experiments

Performed head-to-head model comparisons.

Tested prompt optimization and evaluator calibration.

Video 6 â€” Summary Evaluators

Learned to compute F1 and precision-recall scores.

Analyzed aggregate performance (achieved F1 = 0.86).

Public Dataset:
ğŸ”— LangSmith Dataset
