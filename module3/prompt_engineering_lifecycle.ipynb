{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\Lakshya\\AppData\\Local\\Temp\\ipykernel_20140\\789481971.py:3: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  load_dotenv(dotenv_path=\"D:\\lakshya\\.venv\\Scripts\\intro-to-langsmith\\notebooks\\.env\", override=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"D:\\lakshya\\.venv\\Scripts\\intro-to-langsmith\\notebooks\\.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log a trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To set up tracing to LangSmith using the @traceable decorator, first ensure that the LANGSMITH_TRACING environment variable is set to 'true' and the LANGSMITH_API_KEY is configured. Then, simply decorate any Python function with @traceable to log traces. Remember to use the await keyword when calling wrapped synchronous functions to ensure traces are logged correctly.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app import langsmith_rag\n",
    "\n",
    "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset to evaluate this particular step of our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithConflictError",
     "evalue": "Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langsmith\\utils.py:154\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decorator\n\u001b[1;32m--> 154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_for_status_with_text\u001b[39m(\n\u001b[0;32m    155\u001b[0m     response: Union[requests\u001b[38;5;241m.\u001b[39mResponse, httpx\u001b[38;5;241m.\u001b[39mResponse],\n\u001b[0;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise an error with the response text.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langsmith\\client.py:875\u001b[0m, in \u001b[0;36mrequest_with_retries\u001b[1;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest_with_retries\u001b[39m(\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    871\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    872\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a request with retries.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \n\u001b[0;32m    874\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m--> 875\u001b[0m \u001b[38;5;124;03m        method (str): The HTTP request method.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;124;03m        pathname (str): The pathname of the request URL. Will be appended to the API URL.\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m        request_kwargs (Mapping): Additional request parameters.\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m        stop_after_attempt (int, default=1): The number of attempts to make.\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m        retry_on (Optional[Sequence[Type[BaseException]]]): The exceptions to retry on. In addition to:\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;124;03m            [LangSmithConnectionError, LangSmithAPIError].\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;124;03m        to_ignore (Optional[Sequence[Type[BaseException]]]): The exceptions to ignore / pass on.\u001b[39;00m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03m        handle_response (Optional[Callable[[requests.Response, int], Any]]): A function to handle the response and return whether to continue retrying.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;124;03m        _context (str, default=\"\"): The context of the request.\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;124;03m        **kwargs (Any): Additional keyword arguments to pass to the request.\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m        requests.Response: The response object.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \n\u001b[0;32m    889\u001b[0m \u001b[38;5;124;03m    Raises:\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m        LangSmithAPIError: If a server error occurs.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;124;03m        LangSmithUserError: If the request fails.\u001b[39;00m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;124;03m        LangSmithConnectionError: If a connection error occurs.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m        LangSmithError: If the request fails.\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    895\u001b[0m     request_kwargs \u001b[38;5;241m=\u001b[39m request_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langsmith\\utils.py:156\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decorator\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_for_status_with_text\u001b[39m(\n\u001b[0;32m    155\u001b[0m     response: Union[requests\u001b[38;5;241m.\u001b[39mResponse, httpx\u001b[38;5;241m.\u001b[39mResponse],\n\u001b[1;32m--> 156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise an error with the response text.\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mHTTPError\u001b[0m: [Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLangSmithConflictError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTechnical Questions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Create dataset\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTechnical questions about LangSmith\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Prepare inputs and outputs\u001b[39;00m\n\u001b[0;32m     30\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: q, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: c} \u001b[38;5;28;01mfor\u001b[39;00m q, c, _ \u001b[38;5;129;01min\u001b[39;00m example_dataset]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langsmith\\client.py:3711\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[1;34m(self, dataset_name, description, data_type, inputs_schema, outputs_schema, transformations, metadata)\u001b[0m\n\u001b[0;32m   3709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_stats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3710\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m include_stats\n\u001b[1;32m-> 3711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3712\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_version\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset_version\n\u001b[0;32m   3713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langsmith\\client.py:920\u001b[0m, in \u001b[0;36mrequest_with_retries\u001b[1;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m     request_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    913\u001b[0m logging_filters \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    914\u001b[0m     ls_utils\u001b[38;5;241m.\u001b[39mFilterLangSmithRetry(),\n\u001b[0;32m    915\u001b[0m     ls_utils\u001b[38;5;241m.\u001b[39mFilterPoolFullWarning(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host)),\n\u001b[0;32m    916\u001b[0m ]\n\u001b[0;32m    917\u001b[0m retry_on_: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;241m*\u001b[39m(retry_on \u001b[38;5;129;01mor\u001b[39;00m ()),\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;241m*\u001b[39m(\n\u001b[1;32m--> 920\u001b[0m         ls_utils\u001b[38;5;241m.\u001b[39mLangSmithConnectionError,\n\u001b[0;32m    921\u001b[0m         ls_utils\u001b[38;5;241m.\u001b[39mLangSmithRequestTimeout,  \u001b[38;5;66;03m# 408\u001b[39;00m\n\u001b[0;32m    922\u001b[0m         ls_utils\u001b[38;5;241m.\u001b[39mLangSmithAPIError,  \u001b[38;5;66;03m# 500\u001b[39;00m\n\u001b[0;32m    923\u001b[0m     ),\n\u001b[0;32m    924\u001b[0m )\n\u001b[0;32m    925\u001b[0m to_ignore_: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39m(to_ignore \u001b[38;5;129;01mor\u001b[39;00m ()),)\n\u001b[0;32m    926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mLangSmithConflictError\u001b[0m: Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_dataset = [\n",
    "    (\n",
    "        \"How do I set up tracing to LangSmith with @traceable?\",\n",
    "        \"\"\" 2. Log a trace​\\nOnce you've set up your environment, you can call LangChain runnables as normal.\\nLangSmith will infer the proper tracing config:\\n\\nHow to log and view traces to LangSmith | 🦜️🛠️ LangSmith\\n\\n2. Log a trace​\\nOnce you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\\nLangSmith will then infer the proper tracing config:\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityConceptual GuideHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataTrace JS functions in serverless environmentsMonitoring and automationsTutorialsAdd observability to your LLM applicationEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceObservabilityHow-to GuidesTracingAnnotate code for tracingOn this pageAnnotate code for tracing\\nThere are several ways to log traces to LangSmith.\\ntipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\nUse @traceable / traceable​\\nLangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript.\\nnoteThe LANGSMITH_TRACING environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGSMITH_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\\nTo log traces to a different project, see this section. \\n\\n\"\"\",\n",
    "        \"To set up tracing to LangSmith using the `@traceable` decorator in Python, first ensure that you have the `LANGSMITH_TRACING` environment variable set to 'true' and the `LANGSMITH_API_KEY` set to your API key. Then, you can simply decorate your functions like this:\\n\\n```python\\nfrom langsmith import traceable\\n\\n@traceable\\ndef my_function():\\n    # Your code here\\n    pass\\n```\\n\\nThis will log traces for `my_function` automatically once the necessary environment variables are configured.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How can I use the tracing context manager?\",\n",
    "        \"\"\"Use the trace context manager (Python only)​\\nIn Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where:\\n\\nYou want to log traces for a specific block of code.\\nYou want control over the inputs, outputs, and other attributes of the trace.\\nIt is not feasible to use a decorator or wrapper.\\nAny or all of the above.\\n\\nIn some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.\\nRecently changed behaviorDue to a number of asks for finer-grained control of tracing using the trace context manager,\\nwe changed the behavior of with trace to honor the LANGSMITH_TRACING environment variable in version 0.1.95 of the Python SDK. You can find more details in the release notes.\\nThe recommended way to disable/enable tracing without setting environment variables is to use the with tracing_context context manager, as shown in the example below.\\nPythonTypeScriptThe recommended way to do this in Python is to use the tracing_context context manager. This works for both code annotated with traceable and code within the trace context manager.\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityConceptual GuideHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataTrace JS functions in serverless environmentsMonitoring and automationsTutorialsAdd observability to your LLM applicationEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceObservabilityHow-to GuidesTracingTroubleshoot trace nestingOn this pageTroubleshoot trace nesting\\nWhen tracing with the LangSmith SDK, LangGraph, and LangChain, tracing should automatically propagate the correct context so that code executed within a parent trace will be rendered in the expected location in the UI.\\nIf you see a child run go to a separate trace (and appear on the top level), it may be caused by one of the following known \\\"edge cases\\\".\\nPython​\\nThe following outlines common causes for \\\"split\\\" traces when building with python.\\nContext propagation using asyncio​\\nWhen using async calls (especially with streaming) in Python versions < 3.11, you may encounter issues with trace nesting. This is because Python's asyncio only added full support for passing context in version 3.11.\\nWhy​\\nLangChain and LangSmith SDK use contextvars to propagate tracing information implicitly. In Python 3.11 and above, this works seamlessly. However, in earlier versions (3.8, 3.9, 3.10), asyncio tasks lack proper contextvar support, which can lead to disconnected traces.\\nTo resolve​\\n\\nContext propagation using threading​\\nIt's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib ThreadPoolExecutor by default breaks tracing.\\nWhy​\\nPython's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:\\nTo resolve​\\n\\n\\nUsing LangSmith's ContextThreadPoolExecutor\\nLangSmith provides a ContextThreadPoolExecutor that automatically handles context propagation:\\nfrom langsmith.utils import ContextThreadPoolExecutorfrom langsmith import traceable@traceabledef outer_func():    with ContextThreadPoolExecutor() as executor:        inputs = [1, 2]        r = list(executor.map(inner_func, inputs))@traceabledef inner_func(x):    print(x)outer_func()\\n\\n\\nManually providing the parent run tree\\nAlternatively, you can manually pass the parent run tree to the inner function:\\nfrom langsmith import traceable, get_current_run_treefrom concurrent.futures import ThreadPoolExecutor@traceabledef outer_func():    rt = get_current_run_tree()    with ThreadPoolExecutor() as executor:        r = list(            executor.map(                lambda x: inner_func(x, langsmith_extra={\\\"parent\\\": rt}), [1, 2]            )        )@traceabledef inner_func(x):    print(x)outer_func()\\nIn this approach, we use get_current_run_tree() to obtain the current run tree and pass it to the inner function using the langsmith_extra parameter. \\n\\n \"\"\",\n",
    "        \"You can use the tracing context manager by wrapping the code you want to trace with the `with trace_context:` statement. Here's a simple example:\\n\\n```python\\nfrom langsmith import trace_context\\n\\nwith trace_context:\\n    # Your traceable code here\\n    print(\\\"Tracing this block of code.\\\")\\n``` \\n\\nThis will log traces to LangSmith for the specified code block.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How can I use RunTree?\",\n",
    "        \"\"\"PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \\\"Can you summarize this morning's meetings?\\\"# Create a top-level runpipeline = RunTree(  name=\\\"Chat Pipeline\\\",  run_type=\\\"chain\\\",  inputs={\\\"question\\\": question})# This can be retrieved in a retrieval stepcontext = \\\"During this morning's meeting, we solved all world conflict.\\\"messages = [  { \\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}]# Create a child runchild_llm_run = pipeline.create_child(  name=\\\"OpenAI Call\\\",  run_type=\\\"llm\\\",  inputs={\\\"messages\\\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(  model=\\\"gpt-4o-mini\\\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.postRun()pipeline.end(outputs={\\\"answer\\\": chat_completion.choices[0].message.content})pipeline.postRun()import OpenAI from \\\"openai\\\";import { RunTree } from \\\"langsmith\\\";// This can be a user input to your appconst question = \\\"Can you summarize this morning's meetings?\\\";const pipeline = new RunTree({  name: \\\"Chat Pipeline\\\",  run_type: \\\"chain\\\",  inputs: { question }});await pipeline.postRun();// This can be retrieved in a retrieval stepconst context = \\\"During this morning's meeting, we solved all world conflict.\\\";const messages = [  { role: \\\"system\\\", content: \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { role: \\\"user\\\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({  name: \\\"OpenAI Call\\\",\\n\\nPythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \\\"Can you summarize this morning's meetings?\\\"# Create a top-level runpipeline = RunTree(  name=\\\"Chat Pipeline\\\",  run_type=\\\"chain\\\",  inputs={\\\"question\\\": question})await pipeline.postRun();# This can be retrieved in a retrieval stepcontext = \\\"During this morning's meeting, we solved all world conflict.\\\"messages = [  { \\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}]# Create a child runchild_llm_run = pipeline.create_child(  name=\\\"OpenAI Call\\\",  run_type=\\\"llm\\\",  inputs={\\\"messages\\\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(  model=\\\"gpt-3.5-turbo\\\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.postRun()pipeline.end(outputs={\\\"answer\\\": chat_completion.choices[0].message.content})pipeline.postRun()import OpenAI from \\\"openai\\\";import { RunTree } from \\\"langsmith\\\";// This can be a user input to your appconst question = \\\"Can you summarize this morning's meetings?\\\";const pipeline = new RunTree({  name: \\\"Chat Pipeline\\\",  run_type: \\\"chain\\\",  inputs: { question }});// This can be retrieved in a retrieval stepconst context = \\\"During this morning's meeting, we solved all world conflict.\\\";const messages = [  { role: \\\"system\\\", content: \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { role: \\\"user\\\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({  name: \\\"OpenAI\\n\\nAlternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function.\\n\\nRunTree({  run_type: \\\"llm\\\",  name: \\\"OpenAI Call RunTree\\\",  inputs: { messages },  tags: [\\\"my-tag\\\"],  extra: {metadata: {\\\"my-key\\\": \\\"my-value\\\"}}})await rt.postRun();const chatCompletion = await client.chat.completions.create({  model: \\\"gpt-3.5-turbo\\\",  messages: messages,});// End and submit the runawait rt.end(chatCompletion)await rt.patchRun()from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([(\\\"system\\\", \\\"You are a helpful AI.\\\"),(\\\"user\\\", \\\"{input}\\\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\\\"tags\\\": [\\\"top-level-tag\\\"], \\\"metadata\\\": {\\\"top-level-key\\\": \\\"top-level-value\\\"}})# Tags and metadata can also be passed at runtimechain.invoke({\\\"input\\\": \\\"What is the meaning of life?\\\"}, {\\\"tags\\\": [\\\"shared-tags\\\"], \\\"metadata\\\": {\\\"shared-key\\\": \\\"shared-value\\\"}})import { ChatOpenAI } from \\\"@langchain/openai\\\";import { ChatPromptTemplate } from \\\"@langchain/core/prompts\\\";import { StringOutputParser } from \\\"@langchain/core/output_parsers\\\";const prompt = ChatPromptTemplate.fromMessages([[\\\"system\\\", \\\"You are a helpful AI.\\\"],[\\\"user\\\", \\\"{input}\\\"]])const model = new ChatOpenAI({ modelName: \\\"gpt-3.5-turbo\\\" });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\\\"tags\\\": [\\\"top-level-tag\\\"], \\\"metadata\\\": {\\\"top-level-key\\\": \\\"top-level-value\\\"}});// Tags and metadata can also be \\n\\n\"\"\",\n",
    "        \"You can use `RunTree` by creating a pipeline where you define its name, type, and inputs. For example:\\n\\n```python\\nfrom langsmith.run_trees import RunTree\\n\\npipeline = RunTree(name=\\\"My Pipeline\\\", run_type=\\\"chain\\\", inputs={\\\"question\\\": \\\"Your question here\\\"})\\n```\\n\\nYou can then create child runs and manage them accordingly within the pipeline.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Technical Questions\"\n",
    "\n",
    "# Create dataset\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"Technical questions about LangSmith\"\n",
    ")\n",
    "\n",
    "# Prepare inputs and outputs\n",
    "inputs = [{\"question\": q, \"context\": c} for q, c, _ in example_dataset]\n",
    "outputs = [{\"output\": o} for _, _, o in example_dataset]\n",
    "\n",
    "# Create examples in the dataset\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update our Application to use Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to pretty much define the same RAG application as before - with one crucial improvement.\n",
    "\n",
    "Instead of pulling our `RAG_PROMPT` from utils.py, we're going to connect to the Prompt Hub in LangSmith.\n",
    "\n",
    "Let's add the code snippet that will pull down our prompt that we just iterated on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langsmith import traceable\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "APP_VERSION = 1.0\n",
    "\n",
    "# TODO: Remove this hard-coded prompt and replace it with Prompt Hub\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def get_vector_db_retriever():\n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "    embd = OpenAIEmbeddings()\n",
    "\n",
    "    # If vector store exists, then load it\n",
    "    if os.path.exists(persist_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "    # Otherwise, index LangSmith documents and create new vector store\n",
    "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(ls_docs)\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    # TODO: Let's use our prompt pulled from Prompt Hub instead of manually formatting here!\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    # formatted_prompt = prompt.invoke({\"context\":formatted_docs, \"question\": question})\n",
    "    # messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
    "    return call_openai(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_openai(messages: List[dict]) -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To set up tracing to LangSmith using the @traceable decorator in Python, ensure that the LANGSMITH_TRACING environment variable is set to 'true'. Simply decorate any function with @traceable to log traces automatically. Additionally, set the LANGSMITH_API_KEY environment variable to your API key for proper authentication.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
    "langsmith_rag(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
