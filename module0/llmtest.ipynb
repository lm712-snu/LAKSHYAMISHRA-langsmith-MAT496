{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple RAG](../../images/simple_rag.png)\n",
    "\n",
    "In this notebook, we're going to set up a simple RAG application that we'll be using as we learn more about LangSmith.\n",
    "\n",
    "RAG (Retrieval Augmented Generation) is a popular technique for providing LLMs with relevant documents that will enable them to better answer questions from users. \n",
    "\n",
    "In our case, we are going to index some LangSmith documentation!\n",
    "\n",
    "LangSmith makes it easy to trace any LLM application, no LangChain required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you set your environment variables, including your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline!\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"LANGSMITH_API_KEY\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\Lakshya\\AppData\\Local\\Temp\\ipykernel_17924\\3052250427.py:3: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  load_dotenv(dotenv_path=\"D:\\lakshya\\intro-to-langsmith\\ls-academy\\.env\", override=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"D:\\lakshya\\intro-to-langsmith\\ls-academy\\.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 197/197 [00:43<00:00,  4.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "import sys\n",
    "import langchain_community\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should take a little less than a minute. We are indexing and storing LangSmith documentation in a SKLearn vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a platform designed for building production-grade LLM applications, allowing users to monitor and evaluate their applications for reliability and performance. It provides features for tracing application requests, evaluating application quality over time, and testing prompts with version control. LangSmith is framework agnostic, meaning it can be used with or without LangChain's frameworks.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is LangSmith used for?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"website\": \"www.google.com\"}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is the weather in San Francisco?', additional_kwargs={}, response_metadata={}, id='1874a56a-ea12-4e02-9b07-798edbfc1cc6'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_KNdASjJCOHfByHaY5FMx8Fez', 'function': {'arguments': '{\"city\":\"San Francisco\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 142, 'total_tokens': 166, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CMtSp7gf4kgav2K9dDfq5jeEXn4ud', 'service_tier': 'priority', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--3e7294a5-0e4b-43cd-99f3-5f558408138c-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_KNdASjJCOHfByHaY5FMx8Fez', 'type': 'tool_call'}], usage_metadata={'input_tokens': 142, 'output_tokens': 24, 'total_tokens': 166, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', id='30168ca1-254b-42ed-a5cd-179c04a62666', tool_call_id='call_KNdASjJCOHfByHaY5FMx8Fez'),\n",
       "  AIMessage(content='According to the weather service I checked: \"It\\'s always sunny in San Francisco!\"\\n\\nWould you like more detailed info (current temperature, hourly forecast, 7-day outlook, or weather alerts)?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 111, 'prompt_tokens': 178, 'total_tokens': 289, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CMtSrVeMP63aLM9ujWA6SNSlwR0kf', 'service_tier': 'priority', 'finish_reason': 'stop', 'logprobs': None}, id='run--c9c2ae44-df4e-412c-9a44-54148d966939-0', usage_metadata={'input_tokens': 178, 'output_tokens': 111, 'total_tokens': 289, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LANGSMITH_TRACING='true'\n",
    "LANGSMITH_ENDPOINT='https://api.smith.langchain.com'\n",
    "LANGSMITH_API_KEY='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'\n",
    "LANGSMITH_PROJECT='pr-grumpy-daily-45'\n",
    "OPENAI_API_KEY = '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'\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=\"openai:gpt-5-mini\",\n",
    "    tools=[get_weather],\n",
    "    prompt=\"You are a helpful assistant.\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in San Francisco?\"}]}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
